---
title: "Monitoring"
author: "Andr√© Menezes"
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

# Monitoring

The automatic monitoring method sequentially evaluate the forecasting activity to
detect breakdowns, based on Bayes factor for two models $M_0$ versus $M_1$ with 
same mathematical structure, differing only through the values for $\boldsymbol{\theta}_t$ or simply the discount factors. Let $M_0$ be a standard DLM
without intervention and $M_1$ and alternative model that is introduced to provide
assessment of $M_0$ by comparison. The Bayes' factor for the observed value $y_t$
is given by

$$
H_t = \dfrac{p_0(y_t \mid D_{t-1})}{ p_1(y_t \mid D_{t-1})},
$$
where $p_0$ and $p_1$ are the predictive densities at time $t$ for $M_0$ and 
$M_1$.If $H_t$ is small then the $M_1$ model is preferred. For $k=1, \dots, t$ last consecutive observations $y_t, y_{t-1}, y_{t-k+1}$
the local Bayes factor is given by 
$$
H_t(k) = \prod_{r=t-k+1}^t H_r = \frac{p_0 (y_t, y_{t-1}, \dots, y_{t-k+1})}{p_1 (y_t, y_{t-1}, \dots, y_{t-k+1})}.
$$
and the cumulative Bayes factor $(L_t)$ is
$$
L_t = \min_{1\leq k \leq t} H_t(k),
$$
the minimum at time $t$ is taken at $k=l_t$, with $L_t = H_t(l_t)$ and $l_t$ 
being a integer given by
$$
l_t = (1+l_{t-1}) I(L_{t-1} < 1) + I(L_{t-1} \geq 1),
$$
where $I(\cdot)$ is a indicator function. 

Basically, $H_t$ is initially used to 
indicate if $y_t$ is a outlier when $H_t < \tau$ (which represent preference for $M_1$).
However a small Bayes factor may indicate the start of a regime change, in this case
we need to accumulate evidences. For this $L_t$ and $l_t$ are used. The automatic
detection is done following the steps

* If $H_t \leq \tau$, then $y_t$ is a outlier and is omitted from the analysis.
* If $H_t > \tau$, we must look at $L_t$ for cumulative evidence against $M_0$. 
    + If $L_t < \tau$ or $l_t > 2$ then a parametric chance is detected $M_1$ is adopted.

It is also possible to consider two alternative models $M_1$ and $M_2$, this is useful
for identification of outliers/regime change in two directions. 

## The `Monitoring` class

The `Monitoring` class implements automatic methods of sequentially monitoring
the forecasting activity of DLM in order to detect breakdowns. The model performance is purely based on statistical measures related to model.

An instance of `Monitoring` class can be initialized as follows:

```{python monitoring-initialize-example, echo=TRUE, eval=FALSE}
from pybats_detection.monitor import Monitoring
monitoring_learning = Monitoring(
    mod: pybats.dglm.dlm, prior_length: int = 10, bilateral: bool = False,
    smooth: bool = True, interval: bool = True, level: float = 0.05)
```
where `mod`, `interval`, `level`, and `smooth`  have the same meaning as
in `Intervention` class, `prior_length` is an integer that indicates the
number of prior observations with the monitor off, and
`bilateral` is a Boolean that performs bilateral monitoring if
`True`, otherwise unilateral monitoring.


The fit method of `Monitoring` has the following arguments:
```{python monitoring-fit, echo=TRUE, eval=FALSE}
monitoring_res = monitoring_learning.fit(
     y: pandas.Series, X: pandas.DataFrame = None,
     h: int = 4, tau: float = 0.135,
     discount_factors: dict = {"trend": 0.10, "seasonal": 0.90,
                               "reg": 0.98},
     distr: str = "normal", type: str = "location", verbose: bool = True)
```
where

- `h` is the location or scale shift for alternative distribution.
    
- `tau` is the threshold for Bayes' factors, indicating the lower limit
on acceptability of $L_t$. `tau` lies on $(0, 1)$, values between $0.1$ and
$0.2$ being most appropriate.

- `discount_factors` is a dictionary with exceptional discount factors values
to increase the uncertainty about state space parameter, when the monitor
detects points of intervention. The dictionary should contain values with the
following keys representing the model blocks:
`trend`: level and growth; `seasonal`: seasonality; and `reg`: regressors.

- `dist` is the  Bayes' factors distribution family. It could be `"normal"` or
`"tstudent"`.

- `type` is the alternative model use to compute the Bayes' factors.
It could be `"location"` to detect change in the location of the
distribution or `"scale"` to detect changes in the scale/dispersion
of the predictive distribution.

- `verbose` is a Boolean value that if `True` prints the detection of monitor.

As in the `Intervention` class the output object `monitoring_res` has the same
similar dictionary structure.

## Examples
The effectiveness of the `Monitoring` class is demonstrated in this section
using time series with irregular changes and outliers.
Smaller discount factor values are used to increase the state parameter
uncertainty when a change is regarded as exceptional, which accelerates model
adaption.

### Simulate example

For the first $40$ observations, the following $20$, and the last $40$,
respectively, this simulated data was generated using a normal distribution,
$N[\mu,\sigma^2]$, with $\mu = 100, 104$ and $98$ and $\sigma^2=0.8,0.5$
and $0.5$. The model was defined and simulated using the code:

```{python simulate-data}
np.random.seed(66)
y1 = np.random.normal(loc=100, scale=0.8, size=40)
y2 = np.random.normal(loc=104, scale=0.5, size=20)
y3 = np.random.normal(loc=98, scale=0.5, size=20)
y = np.concatenate([y1, y2, y3])
t = np.arange(0, len(y)) + 1
df_simulated = pd.DataFrame({"t": t, "y": y})
```

```{python simulate-model}
a = np.array([100])
R = np.eye(1)
R[[0]] = 100
mod = dlm(a, R, ntrend=1, deltrend=0.95)
```

The sequential learning with and without the monitor is performed as follows:
\footnotesize
```{python simulate-fitting}
monitor = Monitoring(mod=mod)
fit_monitor = monitor.fit(y=df_simulated["y"],
                          bilateral=True, h=4, tau=0.135,
                          discount_factors={"trend": 0.10},
                          verbose=True)
```
\normalsize

```{python simulate-organizing, include=FALSE}
fit_no_monitor = monitor.fit(y=df_simulated["y"], h=100000)
data_predictive_no_monitor = fit_no_monitor.get("filter").get("predictive")
data_predictive_monitor = fit_monitor.get("filter").get("predictive")

data_predictive_no_monitor["monitor"] = False
data_predictive_monitor["monitor"] = True
data_predictive = pd.concat(
    [data_predictive_no_monitor, data_predictive_monitor])
```

Evidence was found against  model  $M_0$  at $t = 41$ and $t = 61$, with
$L_{41} =6.85\, e^{-6}$, $L_ 61 =2.23\, e^{-4}$, and $L_{61} =2.23\, e^{-4}$,
both with $l_ t = 1$, indicating a possible _outlier_. With the arrival of the
following observations, a regime change is recognized by the monitor.
The interventions performed can be observed in Figure below, where we can see
that the model with monitoring quickly adapts to the regime changes (B),
compared to the one without monitoring (A).

```{r simulate-plot-predictive, include=FALSE, echo=FALSE, fig.cap="One-step-ahead forecasts with  95\\% credible interval for the simulated data. \\textbf{A}: without monitoring. \\textbf{B}: with monitoring. Observations represented by $\\times$ indicate instants with automatic intervention", warnings = FALSE, out.width = "100%"}
data_predictive <- py$data_predictive |> 
  mutate(which_shape = what_detected != "nothing" & what_detected != "")

y_lim <- c(min(filter(data_predictive, t > 8)$ci_lower),
           max(filter(data_predictive, t > 8)$ci_upper))

p_with <- data_predictive |> 
  filter(t > 8, monitor) |> 
  ggplot(aes(x = t, y = y)) +
  geom_point(aes(shape = which_shape), show.legend = FALSE, size = 2) +
  geom_line(aes(y = f), col = blue, size = 1.2) +
  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), col = "grey69",
              alpha = 0.2) +
  scale_shape_manual(values = c(19, 4)) +
  scale_x_continuous(breaks = scales::pretty_breaks(8)) +
  scale_y_continuous(limits = y_lim, breaks = scales::pretty_breaks(6)) +
  labs(x = "Time", y = "y")
p_without <- data_predictive |> 
  filter(t > 8, !monitor) |> 
  ggplot(aes(x = t, y = y)) +
  geom_point(aes(shape = which_shape), show.legend = FALSE, size = 2) +
  geom_line(aes(y = f), col = blue, size = 1.2) +
  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), col = "grey69",
              alpha = 0.2) +
  scale_shape_manual(values = c(19, 4)) +
  scale_x_continuous(breaks = scales::pretty_breaks(8)) +
  scale_y_continuous(limits = y_lim, breaks = scales::pretty_breaks(6)) +
  labs(x = "Time", y = "y")
p_grid <- plot_grid(
  p_without, p_with, ncol = 2, labels = "AUTO", label_size = 20)
p_grid
```



### Telephone Calls 

The telephone calls data set concerns the monthly average number of phone calls
in Cincinnati, USA. This data features three levels of
modifications. The first was in early 1968, with three months of impact;
the second was in the middle of 1973, less significant; and the third was in
early 1974, more lasting. This data set is available in `pybats-detection` and
can be loaded as follows:

```{python load-telephone_calls}
telephone_calls = load_telephone_calls()
```

```{r plot-calls, echo=FALSE, fig.cap="Average daily telephone calls", warnings = FALSE, out.width = "100%"}
df_calls <- py$telephone_calls %>% 
  mutate(time = as.Date(time))
p_calls <- ggplot(df_calls, aes(x = time, y = average_daily_calls)) +
  geom_point(size = 3) +
  scale_y_continuous(breaks = scales::pretty_breaks(10)) +
  scale_x_date(breaks = scales::pretty_breaks(8), date_labels = "%b/%Y") +
  labs(x = "Time", y = "Average daily calls") + 
  theme(axis.text = element_text(size = 20), 
        axis.title = element_text(size = 20))
p_calls
```

It is decided to use the linear growth model to explain this phenomenon.
We set, as usual, vague prior distributions for the level and growth parameters.

```{python telephone-model}
a = np.array([350, 0])
R = np.eye(2)
np.fill_diagonal(R, val=[100])
mod = dlm(a, R, ntrend=2, deltrend=0.90)
```

```{python telephone-fit-without-monitor, include=FALSE}
fit_no_monitor =  Monitoring(mod=mod).fit(y=telephone_calls["average_daily_calls"],
                             h=10000, bilateral=True, prior_length=40)
```

Then, the Monitor class is initialized and the `fit` method is used to
update the model sequentially, taking into account the automatic monitoring.
Note that the discount factor for the level component has dropped from $0.90$ to
$0.20$, while that for the growth remains the same.

\footnotesize
```{python telephone-fit-monitor}
monitor = Monitoring(mod=mod)
fit_monitor = monitor.fit(y=telephone_calls["average_daily_calls"], h=4,
                          tau=0.135,
                          discount_factors={"trend": [0.20, 0.90]},
                          bilateral=True, prior_length=40)   
```
\normalsize

```{python telephone-results, include=FALSE}
# Organize the results
data_predictive_no_monitor = fit_no_monitor.get(
    "filter").get("predictive").copy()
data_predictive_no_monitor["monitor"] = False
data_predictive_monitor = fit_monitor.get("filter").get("predictive").copy()
data_predictive_monitor["monitor"] = True
data_predictive = pd.concat(
    [data_predictive_no_monitor, data_predictive_monitor])
data_predictive = data_predictive.join(telephone_calls)
```



A summary of the changes detection (outlier or parametric change) is printed
with the corresponding values for $H_t$, $L_t$ and $l_t$.
A comparison between the model with and without the automatic monitoring is
shown throughout the one-step-ahead forecasts and the corresponding 95%
credible interval in Figure below.

```{r telephone-plot-predictive, echo=FALSE, fig.cap="One-step-ahead forecasts  with 95\\% credible interval for the telephone calls data. \\textbf{A}: without monitoring. \\textbf{B}: with monitoring. Observations represented by $\\times$ indicate instants with intervention.", warnings = FALSE, out.width = "100%"}
data_predictive <- py$data_predictive |> 
  mutate(time = as.Date(time)) |> 
  as_tibble()

data_predictive <- data_predictive |> 
  mutate(which_shape = what_detected != "nothing" & what_detected != "")
p <- 2
ylim <- c(min(filter(data_predictive, t > 2 * p)$ci_lower),
          max(filter(data_predictive, t > 2 * p)$ci_upper))

p_monitor <- data_predictive |> 
  filter(monitor, t > 2 * p) |> 
  ggplot(aes(x = time, y = average_daily_calls)) +
  geom_point(aes(shape = which_shape), show.legend = FALSE, size = 1) +
  geom_line(aes(y = f), col = blue) +
  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), col = "grey69", alpha = 0.2) +
  scale_shape_manual(values = c(19, 4)) +
  # scale_color_manual(values = c(blue)) +
  scale_x_date(breaks = scales::pretty_breaks(10), date_labels = "%b/%Y") +
  scale_y_continuous(breaks = scales::pretty_breaks(6), limits = ylim) +
  labs(x = "Time", y = "Average daily calls")

p_no_monitor <- data_predictive |> 
  filter(!monitor, t > 2 * p) |> 
  ggplot(aes(x = time, y = average_daily_calls)) +
  geom_point(aes(shape = which_shape), show.legend = FALSE, size = 1) +
  geom_line(aes(y = f), col = blue) +
  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), col = "grey69", alpha = 0.2) +
  scale_shape_manual(values = c(19, 4)) +
  # scale_color_manual(values = c(blue)) +
  scale_x_date(breaks = scales::pretty_breaks(10), date_labels = "%b/%Y") +
  scale_y_continuous(breaks = scales::pretty_breaks(6), limits = ylim) +
  labs(x = "Time", y = "Average daily calls")

p_grid <- plot_grid(p_no_monitor, p_monitor, ncol = 1,
                    labels = "AUTO", label_size = 16)
p_grid
```

It is observed that there is a strong difference in the forecasts between the
models with and without automatic monitoring. In particular, the adaptation for
the future of the model without monitoring is quite poor, which generates large
and imprecise credible intervals. Therefore, for forecasting purposes, the
model with monitoring is well adapted to the level changes.

### Aditional simulation examples

#### Level change

```{python simulating-level-change-data}
np.random.seed(66)
rdlm = RandomDLM(n=50, V=0.1, W=0.005)
df_simulated = rdlm.level(
  start_level=100,
  dict_shift={"t": [40],
    "level_mean_shift": [1],
    "level_var_shift": [1]})
df_simulated.loc[40:50, "y"] = 101 + np.random.normal(0, 0.2, 10)
```

```{r plot-level-change-data, echo=FALSE, fig.cap="Simulated data with level change", out.width = "100%"}
ggplot(py$df_simulated, aes(x = t, y = y)) +
  geom_point(size=3) +
  scale_x_continuous(breaks = scales::pretty_breaks(10)) +
  scale_y_continuous(breaks = scales::pretty_breaks(10))
```

```{python fit-level-change-data}
a = np.array([100])
R = np.eye(1)
R[[0]] = 100
mod = dlm(a, R, ntrend=1, deltrend=0.9)

# Fit without monitoring
fit_without_monitor = Smoothing(mod=mod).fit(y=df_simulated["y"])
df_res = fit_without_monitor.get("filter").get("predictive")

# Fit with monitoring
monitor = Monitoring(mod=mod)
fit_monitor = monitor.fit(y=df_simulated["y"], h=3, tau=0.135, 
                          discount_factors={"trend": 0.10})
df_tmp = fit_monitor.get("filter").get("predictive")
df_res["monitor"] = False
df_tmp["monitor"] = True
cols_ord = ["t", "y", "f", "q", "ci_lower", "ci_upper", "monitor", "e",
            "H", "L", "l"]
df_res = pd.concat([df_res, df_tmp[cols_ord]]).reset_index(drop=True)
```

```{r plot-fit-level-change-data, echo=FALSE, fig.cap="One-step-ahead forecasts for the simulate data with level change. \\textbf{A}: without monitoring. \\textbf{B}: with monitoring.", out.width="100%"}
p1 = py$df_res |>
  filter(t > 5, !monitor) |>
  ggplot(aes(x = t)) +
  geom_point(aes(y = y), size = 2) +
  geom_line(aes(y = f), col = blue) +
  # geom_ribbon(aes(ymin=ci_lower, ymax=ci_upper), alpha=.25, colour = 'grey40') +
  scale_x_continuous(breaks = scales::pretty_breaks(10)) +
  scale_y_continuous(breaks = scales::pretty_breaks(10))

p2 = py$df_res |>
  filter(t > 5, monitor) |>
  ggplot(aes(x = t)) +
  geom_point(aes(y = y), size = 2) +
  geom_line(aes(y = f), col = blue) +
  # geom_ribbon(aes(ymin=ci_lower, ymax=ci_upper), alpha=.25, colour = 'grey40') +
  scale_x_continuous(breaks = scales::pretty_breaks(10)) +
  scale_y_continuous(breaks = scales::pretty_breaks(10))

grid_p = plot_grid(p1, p2, labels = "AUTO", ncol = 1, label_size = 16)
grid_p
```

#### Outliers

```{python simulating-outliers-data}
np.random.seed(66)
rdlm = RandomDLM(n=50, V=0.1, W=0.01)
df_simulated = rdlm.level(
  start_level=100,
  dict_shift={"t": [10, 11, 20, 21, 30, 31, 40, 41],
    "level_mean_shift": [2, -2, 3, -3, 3.4, -3.4, 3, -3],
    "level_var_shift": [1, 1, 1, 1, 1, 1, 1, 1]})
```

```{r plot-outliers-data, echo=FALSE, out.width = "100%"}
ggplot(py$df_simulated, aes(x = t, y = y)) +
  geom_point() +
  scale_x_continuous(breaks = scales::pretty_breaks(20)) +
  scale_y_continuous(breaks = scales::pretty_breaks(6))
```

```{python fit-outliers-data}
a = np.array([100])
R = np.eye(1)
R[[0]] = 100
mod = dlm(a, R, ntrend=1, deltrend=0.9)

# Fit without monitoring
fit_without_monitor = Smoothing(mod=mod).fit(y=df_simulated["y"])
df_res = fit_without_monitor.get("filter").get("predictive")

# Fit with monitoring
monitor = Monitoring(mod=mod)
fit_monitor = monitor.fit(y=df_simulated["y"], h=4, tau=0.135, 
                          discount_factors={"trend": 0.10})
df_tmp = fit_monitor.get("filter").get("predictive")
df_res["monitor"] = False
df_tmp["monitor"] = True

# Append
cols_ord = ["t", "monitor", "y", "f", "q", "ci_lower", "ci_upper"]
df_res = pd.concat([df_res[cols_ord], df_tmp[cols_ord]]).reset_index(drop=True)
```


```{r plot-fit-outliers-data, echo=FALSE, fig.cap="One-step-ahead forecasts for the simulate data with outliers. \\textbf{A}: without monitoring. \\textbf{B}: with monitoring.", out.width = "100%"}
p1 = py$df_res |>
  filter(t > 5, !monitor) |>
  ggplot(aes(x = t)) +
  geom_point(aes(y = y), size = 2) +
  geom_line(aes(y = f), col = blue) +
  # geom_ribbon(aes(ymin=ci_lower, ymax=ci_upper), alpha=.25, colour = 'grey40') +
  scale_x_continuous(breaks = scales::pretty_breaks(8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(6))

p2 = py$df_res |>
  filter(t > 5, monitor) |>
  ggplot(aes(x = t)) +
  geom_point(aes(y = y), size = 2) +
  geom_line(aes(y = f), col =  blue) +
  # geom_ribbon(aes(ymin=ci_lower, ymax=ci_upper), alpha=.25, colour = 'grey40') +
  scale_x_continuous(breaks = scales::pretty_breaks(8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(6))

grid_p = plot_grid(p1, p2, labels = "AUTO", ncol = 1, label_size = 16)
grid_p
```


#### Outlier and Level Change 

```{python simulating-outlier-level-change-data}
np.random.seed(66)
rdlm = RandomDLM(n=50, V=0.1, W=0.01)
df_simulated = rdlm.level(
  start_level=100,
  dict_shift={"t": [20, 21, 40],
    "level_mean_shift": [3, -3, 10],
    "level_var_shift": [1, 1, 1]})
```

```{r plot-outlier-level-change-data, echo=FALSE, out.width = "100%"}
ggplot(py$df_simulated, aes(x = t, y = y)) +
  geom_point(size = 2) +
  scale_x_continuous(breaks = scales::pretty_breaks(20)) +
  scale_y_continuous(breaks = scales::pretty_breaks(8))
```


```{python fit-outlier-level-change-data}
a = np.array([100])
R = np.eye(1)
R[[0]] = 100
mod = dlm(a, R, ntrend=1, deltrend=0.9)

# Fit without monitoring
fit_without_monitor = Smoothing(mod=mod).fit(y=df_simulated["y"])
df_res = fit_without_monitor.get("filter").get("predictive")

# Fit with monitoring
monitor = Monitoring(mod=mod)
fit_monitor = monitor.fit(y=df_simulated["y"], h=4, tau=0.135,
                          discount_factors={"trend": 0.10})
df_tmp = fit_monitor.get("filter").get("predictive")
df_res["monitor"] = False
df_tmp["monitor"] = True

# Append
cols_ord = ["t", "monitor", "y", "f", "q", "ci_lower", "ci_upper"]
df_res = pd.concat([df_res[cols_ord], df_tmp[cols_ord]]).reset_index(drop=True)
```


```{r plot-fit-outlier-level-change-data, echo=FALSE, fig.cap="One-step-ahead forecasts for the simulate data with outliers and level change. \\textbf{A}: without monitoring. \\textbf{B}: with monitoring.", out.width = "100%", out.width = "100%"}
p1 = py$df_res |>
  filter(t > 5, !monitor) |>
  ggplot(aes(x = t)) +
  geom_point(aes(y = y), size = 2) +
  geom_line(aes(y = f), col = blue) +
  # geom_ribbon(aes(ymin=ci_lower, ymax=ci_upper), alpha=.25, colour = 'grey40') +
  scale_x_continuous(breaks = scales::pretty_breaks(10)) +
  scale_y_continuous(breaks = scales::pretty_breaks(8))

p2 = py$df_res |>
  filter(t > 5, monitor) |>
  ggplot(aes(x = t)) +
  geom_point(aes(y = y), size = 2) +
  geom_line(aes(y = f), col = blue) +
  # geom_ribbon(aes(ymin=ci_lower, ymax=ci_upper), alpha=.25, colour = 'grey40') +
  scale_x_continuous(breaks = scales::pretty_breaks(10)) +
  scale_y_continuous(breaks = scales::pretty_breaks(8))

grid_p = plot_grid(p1, p2, labels = "AUTO", ncol = 1, label_size = 16)
grid_p
```


#### Outlier and Two Level Change

```{python simulating-outlier-two-level-change-data}
np.random.seed(66)
rdlm = RandomDLM(n=70, V=1, W=0.01)
df_simulated = rdlm.level(
  start_level=100,
  dict_shift={"t": [20, 21, 40, 60],
    "level_mean_shift": [5, -5, 10, 10],
    "level_var_shift": [1, 1, 1, 1]})
```

```{r plot-outlier-two-level-change-data, echo=FALSE, out.width = "100%"}
ggplot(py$df_simulated, aes(x = t, y = y)) +
  geom_point() + geom_line(linetype = "dashed") +
  scale_x_continuous(breaks = scales::pretty_breaks(20)) +
  scale_y_continuous(breaks = scales::pretty_breaks(10))
```


```{python fit-outlier-two-level-change-data}
a = np.array([100])
R = np.eye(1)
R[[0]] = 100
mod = dlm(a, R, ntrend=1, deltrend=0.9)

# Fit without monitoring
fit_without_monitor = Smoothing(mod=mod).fit(y=df_simulated["y"])
df_res = fit_without_monitor.get("filter").get("predictive")

# Fit with monitoring
monitor = Monitoring(mod=mod)
fit_monitor = monitor.fit(y=df_simulated["y"], h=4, tau=0.135,
                          discount_factors={"trend": 0.10})
df_tmp = fit_monitor.get("filter").get("predictive")
df_res["monitor"] = False
df_tmp["monitor"] = True

# Append
cols_ord = ["t", "monitor", "y", "f", "q", "ci_lower", "ci_upper"]
df_res = pd.concat([df_res, df_tmp[cols_ord]]).reset_index(drop=True)
```


```{r plot-fit-outlier-two-level-change-data, echo=FALSE, fig.cap="One-step-ahead forecasts for the simulate data with outliers and two level change. \\textbf{A}: without monitoring. \\textbf{B}: with monitoring.", out.width = "100%"}
p1 = py$df_res |>
  filter(t > 5, !monitor) |>
  ggplot(aes(x = t)) +
  geom_point(aes(y = y), size = 2) +
  geom_line(aes(y = f), col = blue) +
  # geom_ribbon(aes(ymin=ci_lower, ymax=ci_upper), alpha=.25, colour = 'grey40') +
  scale_x_continuous(breaks = scales::pretty_breaks(10)) +
  scale_y_continuous(breaks = scales::pretty_breaks(8))

p2 = py$df_res |>
  filter(t > 5, monitor) |>
  ggplot(aes(x = t)) +
  geom_point(aes(y = y), size = 2) +
  geom_line(aes(y = f), col = blue) +
  # geom_ribbon(aes(ymin=ci_lower, ymax=ci_upper), alpha=.25, colour = 'grey40') +
  scale_x_continuous(breaks = scales::pretty_breaks(10)) +
  scale_y_continuous(breaks = scales::pretty_breaks(8))

grid_p = plot_grid(p1, p2, labels = "AUTO", ncol = 1, label_size = 16)
grid_p
```


